{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug sentence classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 16:35:47.883074 139983411234624 file_utils.py:39] PyTorch version 1.3.1 available.\n",
      "I1210 16:35:47.906023 139983411234624 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm, trange\n",
    "from ftfy import fix_text\n",
    "from collections import defaultdict\n",
    "import torch, io, gzip, json, random, argparse, os\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (BertTokenizer, BertConfig, AdamW, BertForSequenceClassification,\n",
    "        WarmupLinearSchedule)\n",
    "from ftfy import fix_text\n",
    "from arxiv_public_data.config import DIR_BASE, DIR_OUTPUT, DIR_FULLTEXT\n",
    "f_metadata = os.path.join(DIR_BASE, 'arxiv-metadata-oai-2019-03-01.json.gz')\n",
    "\n",
    "#Got these from Matt\n",
    "cat_map = {\n",
    "  \"astro-ph\": \"astro-ph\",\n",
    "  \"cond-mat\": \"cond-mat\",\n",
    "  \"cs\": \"cs\",\n",
    "  \"gr-qc\": \"gr-qc\",\n",
    "  \"hep-ex\": \"hep-ex\",\n",
    "  \"hep-lat\": \"hep-lat\",\n",
    "  \"hep-ph\": \"hep-ph\",\n",
    "  \"hep-th\": \"hep-th\",\n",
    "  \"math-ph\": \"math-ph\",\n",
    "  \"nlin\": \"nlin\",\n",
    "  \"nucl-ex\": \"nucl-ex\",\n",
    "  \"nucl-th\": \"nucl-th\",\n",
    "  \"physics\": \"physics\",\n",
    "  \"quant-ph\": \"quant-ph\",\n",
    "  \"math\": \"math\",\n",
    "  \"q-bio\": \"q-bio\",\n",
    "  \"q-fin\": \"q-fin\",\n",
    "  \"stat\": \"stat\",\n",
    "  \"eess\": \"eess\",\n",
    "  \"econ\": \"econ\",\n",
    "  \"acc-phys\": \"physics.acc-ph\",\n",
    "  \"adap-org\": \"nlin.AO\",\n",
    "  \"alg-geom\": \"math.AG\",\n",
    "  \"ao-sci\": \"physics.ao-ph\",\n",
    "  \"atom-ph\": \"physics.atom-ph\",\n",
    "  \"bayes-an\": \"physics.data-an\",\n",
    "  \"chao-dyn\": \"nlin.CD\",\n",
    "  \"chem-ph\": \"physics.chem-ph\",\n",
    "  \"cmp-lg\": \"cs.CL\",\n",
    "  \"comp-gas\": \"nlin.CG\",\n",
    "  \"dg-ga\": \"math.DG\",\n",
    "  \"funct-an\": \"math.FA\",\n",
    "  \"mtrl-th\": \"cond-mat.mtrl-sci\",\n",
    "  \"patt-sol\": \"nlin.PS\",\n",
    "  \"plasm-ph\": \"physics.plasm-ph\",\n",
    "  \"q-alg\": \"math.QA\",\n",
    "  \"solv-int\": \"nlin.SI\",\n",
    "  \"supr-con\": \"cond-mat.supr-con\"\n",
    "}\n",
    "\n",
    "\n",
    "# I should experiment with and without this\n",
    "def clean_doc(x):\n",
    "    return fix_text(x)\n",
    "\n",
    "\n",
    "def load_data(N, fname, data_type):\n",
    "    \n",
    "    \n",
    "    #MAX_LENS = [50, 250, 500]  #truncate all titles, abstracts, fulltext to this level\n",
    "    #N, data_type = args.N, args.data_type\n",
    "    #if data_type == 'title':\n",
    "    #     MAX_LEN = MAX_LENS[0]\n",
    "    #elif data_type == 'abstract':\n",
    "    #     MAX_LEN = MAX_LENS[1]\n",
    "    #elif data_type == 'fulltext':\n",
    "    #     MAX_LEN = MAX_LENS[2]\n",
    "    \n",
    "    MAX_LEN = 512  #BERT default\n",
    "    input_ids = []\n",
    "    labels, label_dict, ctr = [], {}, 0\n",
    "    attention_masks = []\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    with gzip.open(fname, 'rt', encoding='utf-8') as fin:\n",
    "        for row in fin.readlines():\n",
    "\n",
    "            #Load metadata\n",
    "            m = json.loads(row)\n",
    "\n",
    "            #Build label list\n",
    "            if data_type != 'fulltext':\n",
    "                sentence = clean_doc(m[data_type])\n",
    "            else:\n",
    "                sentence = load_ith_fulltext(i)  ###needs to be filled in\n",
    "                sentence = clean_doc(sentence)\n",
    "\n",
    "            # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "            sentence = \"[CLS] \" + sentence + \" [SEP]\" \n",
    "\n",
    "            #category\n",
    "            category = m['categories'][0].split(' ')[0]\n",
    "\n",
    "            #update cateogies -- apply matt's map\n",
    "            if category in cat_map: category = cat_map[category]\n",
    "\n",
    "            #Then add to the dics\n",
    "            if category not in label_dict:\n",
    "                index = len(label_dict)\n",
    "                label_dict[category] = index  # ex: {'hep-ph':0, 'math.CO:1',,}\n",
    "            else:\n",
    "                index = label_dict[category]\n",
    "            labels.append(index)\n",
    "\n",
    "\n",
    "            #Tokenize\n",
    "            tokenized_sentence = tokenizer.tokenize(sentence)  #Ex: ['the', 'cat', 'ate']\n",
    "\n",
    "            #Convert to IDs + pad\n",
    "            input_id = tokenizer.convert_tokens_to_ids(tokenized_sentence)  #Ex: [1,10,3]\n",
    "            input_id = pad_sequences([input_id], maxlen=MAX_LEN, dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "            input_ids.append(input_id[0])\n",
    "            \n",
    "            #Attention mask\n",
    "            seq_mask = [float(i>0) for i in input_id[0]]\n",
    "            attention_masks.append(seq_mask)\n",
    "            \n",
    "            #Ctr\n",
    "            ctr += 1\n",
    "            if ctr >= N: break\n",
    "                \n",
    "    return np.array(input_ids), attention_masks, labels, label_dict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 16:35:48.762283 139983411234624 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/khev/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & prepping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 16:36:00.801364 139983411234624 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/khev/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1210 16:36:00.802296 139983411234624 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 87,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# classes = 87\n",
      "Finished loading & prepping data\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 16:36:00.900025 139983411234624 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/khev/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1210 16:36:02.637931 139983411234624 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1210 16:36:02.638724 139983411234624 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n",
      "Train loss: 1.0233550996250576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|â–ˆ         | 1/10 [01:13<11:01, 73.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.8930472266674042\n",
      "Train loss: 0.8056669653786553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|â–ˆâ–ˆ        | 2/10 [02:27<09:50, 73.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.7068917477130889\n",
      "Train loss: 0.6907255503204134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [03:43<08:39, 74.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6221694755554199\n",
      "Train loss: 0.596981921394666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [04:57<07:24, 74.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5321426087617874\n",
      "Train loss: 0.5228252749972874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [06:12<06:11, 74.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.45845855951309206\n",
      "Train loss: 0.4673547583818436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [07:29<05:00, 75.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4049462515115738\n",
      "Train loss: 0.4223775040441089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [08:43<03:44, 74.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3710770732164383\n",
      "Train loss: 0.3870085896386041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [09:56<02:28, 74.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3411976957321167\n",
      "Train loss: 0.368580976261033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [11:09<01:13, 73.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.32941774487495423\n",
      "Train loss: 0.36382306390338476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [12:21<00:00, 73.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3289579349756241\n",
      "Training done: evaluating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpu = True\n",
    "if gpu == True: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else: device = torch.device(\"cpu\")\n",
    "\n",
    "#Load and process data\n",
    "print('Loading & prepping data')\n",
    "epochs = 10\n",
    "N, data_type = 1000, 'title'\n",
    "input_ids, attention_masks, labels, label_dict = load_data(N,f_metadata,data_type)\n",
    "print('# classes = {}'.format(len(label_dict)))\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 4\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for\n",
    "# loop with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "print('Finished loading & prepping data')\n",
    "\n",
    "#Model\n",
    "print(\"Loading model\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict))\n",
    "if gpu:\n",
    "    model.cuda()\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "\n",
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "scheduler = WarmupLinearSchedule(\n",
    "    optimizer, warmup_steps=0.1 * num_training_steps,\n",
    "    t_total=num_training_steps)\n",
    "\n",
    "print(\"Beginning training\")\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    tr_loss, total = 0, 0\n",
    "\n",
    "    #Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        tr_loss += loss.item()\n",
    "        #Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total += logits.shape[0]\n",
    "\n",
    "    tr_loss /= 1.0*(total)\n",
    "    print(\"Train loss: {}\".format(tr_loss))\n",
    "    \n",
    "    \n",
    "    #Add the end of every epoch, find val_loss\n",
    "    val_loss, total = 0, 0\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        val_loss += loss.item()\n",
    "        #Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total += logits.shape[0]\n",
    "        \n",
    "    val_loss /= 1.0*total\n",
    "    print('Val loss: {}'.format(val_loss))\n",
    "        \n",
    "        \n",
    "print('Training done: evaluating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare examples for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, gzip, json\n",
    "from arxiv_public_data.config import DIR_BASE, DIR_OUTPUT, DIR_FULLTEXT\n",
    "f_metadata = os.path.join(DIR_BASE, 'arxiv-metadata-oai-2019-03-01.json.gz')\n",
    "fname = f_metadata\n",
    "\n",
    "N = 100\n",
    "cutoff = int(0.9*N)\n",
    "OUTPUT_DIR = '/home/khev/2TB-harddrive/data/hugging-face-summarization/arxiv-abstracts-titles/small/'\n",
    "with gzip.open(fname, 'rt', encoding='utf-8') as fin:\n",
    "    for i,row in enumerate(fin.readlines()):\n",
    "\n",
    "        #Load metadata\n",
    "        m = json.loads(row)\n",
    "        title, abstract = m['title'], m['abstract']\n",
    "        title, abstract = title.replace('\\n',''), abstract.replace('\\n','')\n",
    "        temp = abstract + '\\n \\n @ highlight \\n \\n ' + title  #form for hugging-face\n",
    "        \n",
    "        #Save as train\n",
    "        fname_sav = os.path.join(OUTPUT_DIR,'{:0>3}.txt'.format(i))\n",
    "        fout = open(fname_sav,'w')\n",
    "        fout.write(temp)\n",
    "        fout.close()\n",
    "        \n",
    "        if i >= N: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 6, 9, 8, 7, 4, 5]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spiralMatrix(matrix):\n",
    "    \"\"\"\n",
    "    \n",
    "    IDEAS:\n",
    "    1. Peel off\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    while matrix and matrix[0]:\n",
    "        \n",
    "        #Go right\n",
    "        if matrix[0]:\n",
    "            for i in matrix.pop(0):\n",
    "                results.append(i)\n",
    "            \n",
    "        #Go down\n",
    "        if matrix and matrix[0]:\n",
    "            for row in matrix:\n",
    "                results.append(row.pop(-1))\n",
    "        \n",
    "        #Go left\n",
    "        if matrix and matrix[0]:\n",
    "            for i in reversed(matrix.pop(-1)):\n",
    "                results.append(i)\n",
    "        \n",
    "        #Go up\n",
    "        if matrix and matrix[0]:\n",
    "            for row in reversed(matrix):\n",
    "                results.append(row.pop(0))\n",
    "                \n",
    "    return results\n",
    "\n",
    "\n",
    "M = [\n",
    " [ 1, 2, 3 ],\n",
    " [ 4, 5, 6 ],\n",
    " [ 7, 8, 9 ]\n",
    "]\n",
    "\n",
    "spiralMatrix(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

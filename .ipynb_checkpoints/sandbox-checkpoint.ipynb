{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug sentence classification\n",
    "\n",
    "\n",
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm, trange\n",
    "from ftfy import fix_text\n",
    "from collections import defaultdict\n",
    "import torch, io, gzip, json, random, argparse, os\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (BertTokenizer, BertConfig, AdamW, BertForSequenceClassification,\n",
    "        WarmupLinearSchedule)\n",
    "from ftfy import fix_text\n",
    "from arxiv_public_data.config import DIR_BASE, DIR_OUTPUT, DIR_FULLTEXT\n",
    "f_metadata = os.path.join(DIR_BASE, 'arxiv-metadata-oai-2019-03-01.json.gz')\n",
    "\n",
    "#Got these from Matt\n",
    "cat_map = {\n",
    "  \"astro-ph\": \"astro-ph\",\n",
    "  \"cond-mat\": \"cond-mat\",\n",
    "  \"cs\": \"cs\",\n",
    "  \"gr-qc\": \"gr-qc\",\n",
    "  \"hep-ex\": \"hep-ex\",\n",
    "  \"hep-lat\": \"hep-lat\",\n",
    "  \"hep-ph\": \"hep-ph\",\n",
    "  \"hep-th\": \"hep-th\",\n",
    "  \"math-ph\": \"math-ph\",\n",
    "  \"nlin\": \"nlin\",\n",
    "  \"nucl-ex\": \"nucl-ex\",\n",
    "  \"nucl-th\": \"nucl-th\",\n",
    "  \"physics\": \"physics\",\n",
    "  \"quant-ph\": \"quant-ph\",\n",
    "  \"math\": \"math\",\n",
    "  \"q-bio\": \"q-bio\",\n",
    "  \"q-fin\": \"q-fin\",\n",
    "  \"stat\": \"stat\",\n",
    "  \"eess\": \"eess\",\n",
    "  \"econ\": \"econ\",\n",
    "  \"acc-phys\": \"physics.acc-ph\",\n",
    "  \"adap-org\": \"nlin.AO\",\n",
    "  \"alg-geom\": \"math.AG\",\n",
    "  \"ao-sci\": \"physics.ao-ph\",\n",
    "  \"atom-ph\": \"physics.atom-ph\",\n",
    "  \"bayes-an\": \"physics.data-an\",\n",
    "  \"chao-dyn\": \"nlin.CD\",\n",
    "  \"chem-ph\": \"physics.chem-ph\",\n",
    "  \"cmp-lg\": \"cs.CL\",\n",
    "  \"comp-gas\": \"nlin.CG\",\n",
    "  \"dg-ga\": \"math.DG\",\n",
    "  \"funct-an\": \"math.FA\",\n",
    "  \"mtrl-th\": \"cond-mat.mtrl-sci\",\n",
    "  \"patt-sol\": \"nlin.PS\",\n",
    "  \"plasm-ph\": \"physics.plasm-ph\",\n",
    "  \"q-alg\": \"math.QA\",\n",
    "  \"solv-int\": \"nlin.SI\",\n",
    "  \"supr-con\": \"cond-mat.supr-con\"\n",
    "}\n",
    "\n",
    "\n",
    "# I should experiment with and without this\n",
    "def clean_doc(x):\n",
    "    return fix_text(x)\n",
    "\n",
    "\n",
    "def load_data(N, fname):\n",
    "    #fname ='/home/khev/research/arxiv-public-datasets/arxiv-data/arxiv-metadata-oai-2019-03-01.json.gz'\n",
    "    metadata = []\n",
    "    ctr = 0\n",
    "    with gzip.open(fname, 'rt', encoding='utf-8') as fin:\n",
    "        for row in fin.readlines():\n",
    "            metadata.append(json.loads(row))\n",
    "            ctr += 1\n",
    "            if ctr > N:\n",
    "                break\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def process_data(metadata, data_type='title'):\n",
    "    \"\"\"\n",
    "    data_type \\element ['title', 'abstract']\n",
    "    \"\"\"\n",
    "\n",
    "    sentences, labels, label_dict = [], [], {}\n",
    "    for m in metadata:\n",
    "\n",
    "        #sentences / titles\n",
    "        sentence = clean_doc(m[data_type])\n",
    "        \n",
    "        # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "        sentence = \"[CLS] \" + sentence + \" [SEP]\" \n",
    "        sentences.append(sentence)\n",
    "\n",
    "        #category\n",
    "        category = m['categories'][0].split(' ')[0]\n",
    "\n",
    "        #Take only primary index: 'math.CO' --> 'math'\n",
    "        primaryCategories = False\n",
    "        if primaryCategories:\n",
    "            cutoff = len(category)\n",
    "            try:\n",
    "                cutoff = category.index('.')\n",
    "            except ValueError:\n",
    "                    pass\n",
    "            category = category[:cutoff]\n",
    "        \n",
    "        if category not in label_dict:\n",
    "            index = len(label_dict)\n",
    "            label_dict[category] = index  # e.g. {'hep-ph':2}\n",
    "        else:\n",
    "            index = label_dict[category]\n",
    "        labels.append(index)\n",
    "\n",
    "    return sentences, labels, label_dict\n",
    "\n",
    "\n",
    "def process_data_sub(metadata, data_type='title'):\n",
    "    \"\"\"\n",
    "    Same as above, except I merge categories that are the same\n",
    "    (origianl data in buggy: category names changed over times so have to be fixed)\n",
    "    \n",
    "    data_type='title' or 'abstract' or 'fulltext'\n",
    "   \n",
    "    \"\"\"\n",
    "\n",
    "    sentences, labels, label_dict = [], [], {}\n",
    "    for i, m in enumerate(metadata):\n",
    "\n",
    "        #sentences / titles\n",
    "        if data_type != 'fulltext':\n",
    "            sentence = clean_doc(m[data_type])\n",
    "        else:\n",
    "            sentence = load_ith_fulltext(i)  ###needs to be filled in\n",
    "            sentence = clean_doc(sentence)\n",
    "        \n",
    "        # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "        sentence = \"[CLS] \" + sentence + \" [SEP]\" \n",
    "        sentences.append(sentence)\n",
    "\n",
    "        #category\n",
    "        category = m['categories'][0].split(' ')[0]\n",
    "        \n",
    "        #update cateogies -- apply matt's map\n",
    "        if category in cat_map:\n",
    "            category = cat_map[category]\n",
    "        \n",
    "        #Then add to the dics\n",
    "        if category not in label_dict:\n",
    "            index = len(label_dict)\n",
    "            label_dict[category] = index  # ex: {'hep-ph':0, 'math.CO:1',,}\n",
    "        else:\n",
    "            index = label_dict[category]\n",
    "        labels.append(index)\n",
    "\n",
    "    return sentences, labels, label_dict\n",
    "\n",
    "\n",
    "#N, data_type = 10**7, 'title'\n",
    "#metadata = load_data(N,f_metadata)\n",
    "#sentences, labels, label_dict_new = process_data_sub(metadata, data_type=data_type)\n",
    "#len(label_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, data_type = 10**7, 'title'\n",
    "metadata = load_data(N,f_metadata)\n",
    "sentences, labels, label_dict_old = process_data(metadata, data_type=data_type)\n",
    "len(label_dict_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'astro-ph': 'astro-ph',\n",
       " 'cond-mat': 'cond-mat',\n",
       " 'cs': 'cs',\n",
       " 'gr-qc': 'gr-qc',\n",
       " 'hep-ex': 'hep-ex',\n",
       " 'hep-lat': 'hep-lat',\n",
       " 'hep-ph': 'hep-ph',\n",
       " 'hep-th': 'hep-th',\n",
       " 'math-ph': 'math-ph',\n",
       " 'nlin': 'nlin',\n",
       " 'nucl-ex': 'nucl-ex',\n",
       " 'nucl-th': 'nucl-th',\n",
       " 'physics': 'physics',\n",
       " 'quant-ph': 'quant-ph',\n",
       " 'math': 'math',\n",
       " 'q-bio': 'q-bio',\n",
       " 'q-fin': 'q-fin',\n",
       " 'stat': 'stat',\n",
       " 'eess': 'eess',\n",
       " 'econ': 'econ',\n",
       " 'acc-phys': 'physics.acc-ph',\n",
       " 'adap-org': 'nlin.AO',\n",
       " 'alg-geom': 'math.AG',\n",
       " 'ao-sci': 'physics.ao-ph',\n",
       " 'atom-ph': 'physics.atom-ph',\n",
       " 'bayes-an': 'physics.data-an',\n",
       " 'chao-dyn': 'nlin.CD',\n",
       " 'chem-ph': 'physics.chem-ph',\n",
       " 'cmp-lg': 'cs.CL',\n",
       " 'comp-gas': 'nlin.CG',\n",
       " 'dg-ga': 'math.DG',\n",
       " 'funct-an': 'math.FA',\n",
       " 'mtrl-th': 'cond-mat.mtrl-sci',\n",
       " 'patt-sol': 'nlin.PS',\n",
       " 'plasm-ph': 'physics.plasm-ph',\n",
       " 'q-alg': 'math.QA',\n",
       " 'solv-int': 'nlin.SI',\n",
       " 'supr-con': 'cond-mat.supr-con'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'adap-org' in label_dict_old, 'adap-org' in label_dict_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation\n",
    "\n",
    "When we truncate the titles, do we chop off the special \" [SEP] \" token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1205 16:47:50.308459 139851126974272 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/khev/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes = 43\n",
      "Tokenizing\n"
     ]
    }
   ],
   "source": [
    "N, data_type = 10**2, 'title'\n",
    "metadata = load_data(N,f_metadata)\n",
    "sentences, labels, label_dict = process_data_sub(metadata, data_type=data_type)\n",
    "print('Num classes = {}'.format(len(label_dict)))\n",
    "print('Tokenizing')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512  # BERT pretrained model width\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts] #bert tokenizer\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") #pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the pretrained BERT used MAX_LEN = 512, so we're stuck with this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1205 17:57:26.602200 139851126974272 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/khev/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  101, 17208,  1997, ...,     0,     0,     0],\n",
       "       [  101, 12403,  2869, ...,     0,     0,     0],\n",
       "       [  101,  1996,  6622, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101, 24961,  1011, ...,     0,     0,     0],\n",
       "       [  101,  2006,  1998, ...,     0,     0,     0],\n",
       "       [  101, 19587,  2689, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname ='/home/khev/research/arxiv-public-datasets/arxiv-data/arxiv-metadata-oai-2019-03-01.json.gz'\n",
    "\n",
    "\n",
    "def load_data_better(N, fname, data_type):\n",
    "    \n",
    "    \n",
    "    #MAX_LENS = [50, 250, 500]  #truncate all titles, abstracts, fulltext to this level\n",
    "    #N, data_type = args.N, args.data_type\n",
    "    #if data_type == 'title':\n",
    "    #     MAX_LEN = MAX_LENS[0]\n",
    "    #elif data_type == 'abstract':\n",
    "    #     MAX_LEN = MAX_LENS[1]\n",
    "    #elif data_type == 'fulltext':\n",
    "    #     MAX_LEN = MAX_LENS[2]\n",
    "    \n",
    "    MAX_LEN = 512  #BERT default\n",
    "    input_ids = []\n",
    "    labels, label_dict, ctr = [], {}, 0\n",
    "    attention_masks = []\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    with gzip.open(fname, 'rt', encoding='utf-8') as fin:\n",
    "        for row in fin.readlines():\n",
    "\n",
    "            #Load metadata\n",
    "            m = json.loads(row)\n",
    "\n",
    "            #Build label list\n",
    "            if data_type != 'fulltext':\n",
    "                sentence = clean_doc(m[data_type])\n",
    "            else:\n",
    "                sentence = load_ith_fulltext(i)  ###needs to be filled in\n",
    "                sentence = clean_doc(sentence)\n",
    "\n",
    "            # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "            sentence = \"[CLS] \" + sentence + \" [SEP]\" \n",
    "\n",
    "            #category\n",
    "            category = m['categories'][0].split(' ')[0]\n",
    "\n",
    "            #update cateogies -- apply matt's map\n",
    "            if category in cat_map: category = cat_map[category]\n",
    "\n",
    "            #Then add to the dics\n",
    "            if category not in label_dict:\n",
    "                index = len(label_dict)\n",
    "                label_dict[category] = index  # ex: {'hep-ph':0, 'math.CO:1',,}\n",
    "            else:\n",
    "                index = label_dict[category]\n",
    "            labels.append(index)\n",
    "\n",
    "\n",
    "            #Tokenize\n",
    "            tokenized_sentence = tokenizer.tokenize(sentence)  #Ex: ['the', 'cat', 'ate']\n",
    "\n",
    "            #Convert to IDs + pad\n",
    "            input_id = tokenizer.convert_tokens_to_ids(tokenized_sentence)  #Ex: [1,10,3]\n",
    "            input_id = pad_sequences([input_id], maxlen=MAX_LEN, dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "            input_ids.append(input_id[0])\n",
    "            \n",
    "            #Attention mask\n",
    "            seq_mask = [float(i>0) for i in input_id[0]]\n",
    "            attention_masks.append(seq_mask)\n",
    "            \n",
    "            #Ctr\n",
    "            ctr += 1\n",
    "            if ctr >= N: break\n",
    "                \n",
    "    return np.array(input_ids), attention_masks, labels, label_dict \n",
    "\n",
    "            \n",
    "data_type, N = 'title', 100\n",
    "fname = '/home/khev/research/arxiv-public-datasets/arxiv-data/arxiv-metadata-oai-2019-03-01.json.gz'\n",
    "input_ids, attention_masks, labels, label_dict = load_data_better(N,fname,data_type)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1205 17:57:37.939976 139851126974272 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/khev/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1205 17:57:37.941224 139851126974272 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 43,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1205 17:57:38.029388 139851126974272 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/khev/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & prepping data\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1205 17:57:39.796591 139851126974272 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1205 17:57:39.797329 139851126974272 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training\n",
      "Train loss: 4.254676818847656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-7d7f918b08e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m#evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0macc1_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc3_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc5_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogliklihood_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0macc1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc1_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "if gpu == True: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else: device = torch.device(\"cpu\")\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size, gpu = 2, True\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for\n",
    "# loop with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "print('Loading & prepping data')\n",
    "\n",
    "#Model\n",
    "print(\"Loading model\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict))\n",
    "if gpu:\n",
    "    model.cuda()\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Test and train\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "epochs = 2 \n",
    "\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "\n",
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "scheduler = WarmupLinearSchedule(\n",
    "    optimizer, warmup_steps=0.1 * num_training_steps,\n",
    "    t_total=num_training_steps\n",
    ")\n",
    "\n",
    "print(\"Beginning training\")\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    #Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss)    \n",
    "        #Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    model.eval()\n",
    "\n",
    "    #Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  #Evaluate data for one epoch\n",
    "    acc1, acc3, acc5, logliklihood = 0,0,0,0\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        #evaluate\n",
    "        acc1_temp, acc3_temp, acc5_temp, logliklihood_temp = evaluate(logits,label_ids)\n",
    "\n",
    "        acc1 += acc1_temp\n",
    "        acc3 += acc3_temp\n",
    "        acc5 += acc5_temp\n",
    "        logliklihood += logliklihood_temp\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "acc1 /= nb_eval_steps\n",
    "acc3 /= nb_eval_steps\n",
    "acc5 /= nb_eval_steps\n",
    "\n",
    "num_examples = len(batch)*nb_eval_steps\n",
    "perplexity = 2** (-logliklihood / num_examples / np.log(2))\n",
    "line = '{}: top1, top3, top5, perplexity = {:.2f}, {:.2f}, {:.2f}, {:.2f}'.format(data_type, acc1,acc3,acc5,perplexity)\n",
    "print(line)\n",
    "\n",
    "#Save data\n",
    "DIR_NAME = './output'\n",
    "if not os.path.exists(DIR_NAME):\n",
    "    os.makedirs(DIR_NAME)\n",
    "fname = os.path.join(DIR_NAME[2:],'classification-results-N-{}-{}.txt'.format(N,data_type))\n",
    "np.savetxt(fname,[line],fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32,  0], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101.,  8790.,  2389.,  ...,     0.,     0.,     0.],\n",
       "        [  101., 17208.,  1997.,  ...,     0.,     0.,     0.]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

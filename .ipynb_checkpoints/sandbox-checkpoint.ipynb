{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug sentence classification\n",
    "\n",
    "\n",
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 15:38:43.884996 139938003277632 file_utils.py:39] PyTorch version 1.3.1 available.\n",
      "I1210 15:38:43.928926 139938003277632 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm, trange\n",
    "from ftfy import fix_text\n",
    "from collections import defaultdict\n",
    "import torch, io, gzip, json, random, argparse, os\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (BertTokenizer, BertConfig, AdamW, BertForSequenceClassification,\n",
    "        WarmupLinearSchedule)\n",
    "from ftfy import fix_text\n",
    "from arxiv_public_data.config import DIR_BASE, DIR_OUTPUT, DIR_FULLTEXT\n",
    "f_metadata = os.path.join(DIR_BASE, 'arxiv-metadata-oai-2019-03-01.json.gz')\n",
    "\n",
    "#Got these from Matt\n",
    "cat_map = {\n",
    "  \"astro-ph\": \"astro-ph\",\n",
    "  \"cond-mat\": \"cond-mat\",\n",
    "  \"cs\": \"cs\",\n",
    "  \"gr-qc\": \"gr-qc\",\n",
    "  \"hep-ex\": \"hep-ex\",\n",
    "  \"hep-lat\": \"hep-lat\",\n",
    "  \"hep-ph\": \"hep-ph\",\n",
    "  \"hep-th\": \"hep-th\",\n",
    "  \"math-ph\": \"math-ph\",\n",
    "  \"nlin\": \"nlin\",\n",
    "  \"nucl-ex\": \"nucl-ex\",\n",
    "  \"nucl-th\": \"nucl-th\",\n",
    "  \"physics\": \"physics\",\n",
    "  \"quant-ph\": \"quant-ph\",\n",
    "  \"math\": \"math\",\n",
    "  \"q-bio\": \"q-bio\",\n",
    "  \"q-fin\": \"q-fin\",\n",
    "  \"stat\": \"stat\",\n",
    "  \"eess\": \"eess\",\n",
    "  \"econ\": \"econ\",\n",
    "  \"acc-phys\": \"physics.acc-ph\",\n",
    "  \"adap-org\": \"nlin.AO\",\n",
    "  \"alg-geom\": \"math.AG\",\n",
    "  \"ao-sci\": \"physics.ao-ph\",\n",
    "  \"atom-ph\": \"physics.atom-ph\",\n",
    "  \"bayes-an\": \"physics.data-an\",\n",
    "  \"chao-dyn\": \"nlin.CD\",\n",
    "  \"chem-ph\": \"physics.chem-ph\",\n",
    "  \"cmp-lg\": \"cs.CL\",\n",
    "  \"comp-gas\": \"nlin.CG\",\n",
    "  \"dg-ga\": \"math.DG\",\n",
    "  \"funct-an\": \"math.FA\",\n",
    "  \"mtrl-th\": \"cond-mat.mtrl-sci\",\n",
    "  \"patt-sol\": \"nlin.PS\",\n",
    "  \"plasm-ph\": \"physics.plasm-ph\",\n",
    "  \"q-alg\": \"math.QA\",\n",
    "  \"solv-int\": \"nlin.SI\",\n",
    "  \"supr-con\": \"cond-mat.supr-con\"\n",
    "}\n",
    "\n",
    "\n",
    "# I should experiment with and without this\n",
    "def clean_doc(x):\n",
    "    return fix_text(x)\n",
    "\n",
    "\n",
    "def load_data(N, fname, data_type):\n",
    "    \n",
    "    \n",
    "    #MAX_LENS = [50, 250, 500]  #truncate all titles, abstracts, fulltext to this level\n",
    "    #N, data_type = args.N, args.data_type\n",
    "    #if data_type == 'title':\n",
    "    #     MAX_LEN = MAX_LENS[0]\n",
    "    #elif data_type == 'abstract':\n",
    "    #     MAX_LEN = MAX_LENS[1]\n",
    "    #elif data_type == 'fulltext':\n",
    "    #     MAX_LEN = MAX_LENS[2]\n",
    "    \n",
    "    MAX_LEN = 512  #BERT default\n",
    "    input_ids = []\n",
    "    labels, label_dict, ctr = [], {}, 0\n",
    "    attention_masks = []\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    with gzip.open(fname, 'rt', encoding='utf-8') as fin:\n",
    "        for row in fin.readlines():\n",
    "\n",
    "            #Load metadata\n",
    "            m = json.loads(row)\n",
    "\n",
    "            #Build label list\n",
    "            if data_type != 'fulltext':\n",
    "                sentence = clean_doc(m[data_type])\n",
    "            else:\n",
    "                sentence = load_ith_fulltext(i)  ###needs to be filled in\n",
    "                sentence = clean_doc(sentence)\n",
    "\n",
    "            # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "            sentence = \"[CLS] \" + sentence + \" [SEP]\" \n",
    "\n",
    "            #category\n",
    "            category = m['categories'][0].split(' ')[0]\n",
    "\n",
    "            #update cateogies -- apply matt's map\n",
    "            if category in cat_map: category = cat_map[category]\n",
    "\n",
    "            #Then add to the dics\n",
    "            if category not in label_dict:\n",
    "                index = len(label_dict)\n",
    "                label_dict[category] = index  # ex: {'hep-ph':0, 'math.CO:1',,}\n",
    "            else:\n",
    "                index = label_dict[category]\n",
    "            labels.append(index)\n",
    "\n",
    "\n",
    "            #Tokenize\n",
    "            tokenized_sentence = tokenizer.tokenize(sentence)  #Ex: ['the', 'cat', 'ate']\n",
    "\n",
    "            #Convert to IDs + pad\n",
    "            input_id = tokenizer.convert_tokens_to_ids(tokenized_sentence)  #Ex: [1,10,3]\n",
    "            input_id = pad_sequences([input_id], maxlen=MAX_LEN, dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "            input_ids.append(input_id[0])\n",
    "            \n",
    "            #Attention mask\n",
    "            seq_mask = [float(i>0) for i in input_id[0]]\n",
    "            attention_masks.append(seq_mask)\n",
    "            \n",
    "            #Ctr\n",
    "            ctr += 1\n",
    "            if ctr >= N: break\n",
    "                \n",
    "    return np.array(input_ids), attention_masks, labels, label_dict  \n",
    "\n",
    "\n",
    "\n",
    "#N, data_type = 10**7, 'title'\n",
    "#metadata = load_data(N,f_metadata)\n",
    "#sentences, labels, label_dict_new = process_data_sub(metadata, data_type=data_type)\n",
    "#len(label_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, data_type = 10**7, 'title'\n",
    "metadata = load_data(N,f_metadata)\n",
    "sentences, labels, label_dict_old = process_data(metadata, data_type=data_type)\n",
    "len(label_dict_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'astro-ph': 'astro-ph',\n",
       " 'cond-mat': 'cond-mat',\n",
       " 'cs': 'cs',\n",
       " 'gr-qc': 'gr-qc',\n",
       " 'hep-ex': 'hep-ex',\n",
       " 'hep-lat': 'hep-lat',\n",
       " 'hep-ph': 'hep-ph',\n",
       " 'hep-th': 'hep-th',\n",
       " 'math-ph': 'math-ph',\n",
       " 'nlin': 'nlin',\n",
       " 'nucl-ex': 'nucl-ex',\n",
       " 'nucl-th': 'nucl-th',\n",
       " 'physics': 'physics',\n",
       " 'quant-ph': 'quant-ph',\n",
       " 'math': 'math',\n",
       " 'q-bio': 'q-bio',\n",
       " 'q-fin': 'q-fin',\n",
       " 'stat': 'stat',\n",
       " 'eess': 'eess',\n",
       " 'econ': 'econ',\n",
       " 'acc-phys': 'physics.acc-ph',\n",
       " 'adap-org': 'nlin.AO',\n",
       " 'alg-geom': 'math.AG',\n",
       " 'ao-sci': 'physics.ao-ph',\n",
       " 'atom-ph': 'physics.atom-ph',\n",
       " 'bayes-an': 'physics.data-an',\n",
       " 'chao-dyn': 'nlin.CD',\n",
       " 'chem-ph': 'physics.chem-ph',\n",
       " 'cmp-lg': 'cs.CL',\n",
       " 'comp-gas': 'nlin.CG',\n",
       " 'dg-ga': 'math.DG',\n",
       " 'funct-an': 'math.FA',\n",
       " 'mtrl-th': 'cond-mat.mtrl-sci',\n",
       " 'patt-sol': 'nlin.PS',\n",
       " 'plasm-ph': 'physics.plasm-ph',\n",
       " 'q-alg': 'math.QA',\n",
       " 'solv-int': 'nlin.SI',\n",
       " 'supr-con': 'cond-mat.supr-con'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'adap-org' in label_dict_old, 'adap-org' in label_dict_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation\n",
    "\n",
    "When we truncate the titles, do we chop off the special \" [SEP] \" token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1205 16:47:50.308459 139851126974272 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/khev/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes = 43\n",
      "Tokenizing\n"
     ]
    }
   ],
   "source": [
    "N, data_type = 10**2, 'title'\n",
    "metadata = load_data(N,f_metadata)\n",
    "sentences, labels, label_dict = process_data_sub(metadata, data_type=data_type)\n",
    "print('Num classes = {}'.format(len(label_dict)))\n",
    "print('Tokenizing')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512  # BERT pretrained model width\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts] #bert tokenizer\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") #pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the pretrained BERT used MAX_LEN = 512, so we're stuck with this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 15:38:46.545981 139938003277632 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/khev/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "batch_size, epochs = 12, 2\n",
    "N, data_type = 100, 'title'\n",
    "f_metadata = os.path.join(DIR_BASE, 'arxiv-metadata-oai-2019-03-01.json.gz')\n",
    "input_ids, attention_masks, labels, label_dict = load_data(N,f_metadata,data_type)  \n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for\n",
    "# loop with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 15:39:16.653043 139938003277632 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/khev/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1210 15:39:16.654027 139938003277632 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 43,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 15:39:16.794534 139938003277632 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/khev/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1210 15:39:19.735785 139938003277632 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I1210 15:39:19.736549 139938003277632 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "print(\"Loading model\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_dict))\n",
    "\n",
    "#Optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}]\n",
    "\n",
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "scheduler = WarmupLinearSchedule(\n",
    "    optimizer, warmup_steps=0.1 * num_training_steps,\n",
    "    t_total=num_training_steps)\n",
    "\n",
    "\n",
    "#train\n",
    "epochs = 2\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loss_set = []\n",
    "device = torch.device('cpu')\n",
    "for _ in range(epochs):\n",
    "    \n",
    "    #train per batch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss)  \n",
    "        \n",
    "        #Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

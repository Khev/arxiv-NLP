{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug sentence classification\n",
    "\n",
    "\n",
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 15:47:19.839966 140225670215488 file_utils.py:39] PyTorch version 1.3.1 available.\n",
      "I1210 15:47:19.862981 140225670215488 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm, trange\n",
    "from ftfy import fix_text\n",
    "from collections import defaultdict\n",
    "import torch, io, gzip, json, random, argparse, os\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (BertTokenizer, BertConfig, AdamW, BertForSequenceClassification,\n",
    "        WarmupLinearSchedule)\n",
    "from ftfy import fix_text\n",
    "from arxiv_public_data.config import DIR_BASE, DIR_OUTPUT, DIR_FULLTEXT\n",
    "f_metadata = os.path.join(DIR_BASE, 'arxiv-metadata-oai-2019-03-01.json.gz')\n",
    "\n",
    "#Got these from Matt\n",
    "cat_map = {\n",
    "  \"astro-ph\": \"astro-ph\",\n",
    "  \"cond-mat\": \"cond-mat\",\n",
    "  \"cs\": \"cs\",\n",
    "  \"gr-qc\": \"gr-qc\",\n",
    "  \"hep-ex\": \"hep-ex\",\n",
    "  \"hep-lat\": \"hep-lat\",\n",
    "  \"hep-ph\": \"hep-ph\",\n",
    "  \"hep-th\": \"hep-th\",\n",
    "  \"math-ph\": \"math-ph\",\n",
    "  \"nlin\": \"nlin\",\n",
    "  \"nucl-ex\": \"nucl-ex\",\n",
    "  \"nucl-th\": \"nucl-th\",\n",
    "  \"physics\": \"physics\",\n",
    "  \"quant-ph\": \"quant-ph\",\n",
    "  \"math\": \"math\",\n",
    "  \"q-bio\": \"q-bio\",\n",
    "  \"q-fin\": \"q-fin\",\n",
    "  \"stat\": \"stat\",\n",
    "  \"eess\": \"eess\",\n",
    "  \"econ\": \"econ\",\n",
    "  \"acc-phys\": \"physics.acc-ph\",\n",
    "  \"adap-org\": \"nlin.AO\",\n",
    "  \"alg-geom\": \"math.AG\",\n",
    "  \"ao-sci\": \"physics.ao-ph\",\n",
    "  \"atom-ph\": \"physics.atom-ph\",\n",
    "  \"bayes-an\": \"physics.data-an\",\n",
    "  \"chao-dyn\": \"nlin.CD\",\n",
    "  \"chem-ph\": \"physics.chem-ph\",\n",
    "  \"cmp-lg\": \"cs.CL\",\n",
    "  \"comp-gas\": \"nlin.CG\",\n",
    "  \"dg-ga\": \"math.DG\",\n",
    "  \"funct-an\": \"math.FA\",\n",
    "  \"mtrl-th\": \"cond-mat.mtrl-sci\",\n",
    "  \"patt-sol\": \"nlin.PS\",\n",
    "  \"plasm-ph\": \"physics.plasm-ph\",\n",
    "  \"q-alg\": \"math.QA\",\n",
    "  \"solv-int\": \"nlin.SI\",\n",
    "  \"supr-con\": \"cond-mat.supr-con\"\n",
    "}\n",
    "\n",
    "\n",
    "# I should experiment with and without this\n",
    "def clean_doc(x):\n",
    "    return fix_text(x)\n",
    "\n",
    "\n",
    "def load_data(N, fname, data_type):\n",
    "    \n",
    "    \n",
    "    #MAX_LENS = [50, 250, 500]  #truncate all titles, abstracts, fulltext to this level\n",
    "    #N, data_type = args.N, args.data_type\n",
    "    #if data_type == 'title':\n",
    "    #     MAX_LEN = MAX_LENS[0]\n",
    "    #elif data_type == 'abstract':\n",
    "    #     MAX_LEN = MAX_LENS[1]\n",
    "    #elif data_type == 'fulltext':\n",
    "    #     MAX_LEN = MAX_LENS[2]\n",
    "    \n",
    "    MAX_LEN = 512  #BERT default\n",
    "    input_ids = []\n",
    "    labels, label_dict, ctr = [], {}, 0\n",
    "    attention_masks = []\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    with gzip.open(fname, 'rt', encoding='utf-8') as fin:\n",
    "        for row in fin.readlines():\n",
    "\n",
    "            #Load metadata\n",
    "            m = json.loads(row)\n",
    "\n",
    "            #Build label list\n",
    "            if data_type != 'fulltext':\n",
    "                sentence = clean_doc(m[data_type])\n",
    "            else:\n",
    "                sentence = load_ith_fulltext(i)  ###needs to be filled in\n",
    "                sentence = clean_doc(sentence)\n",
    "\n",
    "            # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "            sentence = \"[CLS] \" + sentence + \" [SEP]\" \n",
    "\n",
    "            #category\n",
    "            category = m['categories'][0].split(' ')[0]\n",
    "\n",
    "            #update cateogies -- apply matt's map\n",
    "            if category in cat_map: category = cat_map[category]\n",
    "\n",
    "            #Then add to the dics\n",
    "            if category not in label_dict:\n",
    "                index = len(label_dict)\n",
    "                label_dict[category] = index  # ex: {'hep-ph':0, 'math.CO:1',,}\n",
    "            else:\n",
    "                index = label_dict[category]\n",
    "            labels.append(index)\n",
    "\n",
    "\n",
    "            #Tokenize\n",
    "            tokenized_sentence = tokenizer.tokenize(sentence)  #Ex: ['the', 'cat', 'ate']\n",
    "\n",
    "            #Convert to IDs + pad\n",
    "            input_id = tokenizer.convert_tokens_to_ids(tokenized_sentence)  #Ex: [1,10,3]\n",
    "            input_id = pad_sequences([input_id], maxlen=MAX_LEN, dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
    "            input_ids.append(input_id[0])\n",
    "            \n",
    "            #Attention mask\n",
    "            seq_mask = [float(i>0) for i in input_id[0]]\n",
    "            attention_masks.append(seq_mask)\n",
    "            \n",
    "            #Ctr\n",
    "            ctr += 1\n",
    "            if ctr >= N: break\n",
    "                \n",
    "    return np.array(input_ids), attention_masks, labels, label_dict  \n",
    "\n",
    "\n",
    "\n",
    "#N, data_type = 10**7, 'title'\n",
    "#metadata = load_data(N,f_metadata)\n",
    "#sentences, labels, label_dict_new = process_data_sub(metadata, data_type=data_type)\n",
    "#len(label_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, data_type = 10**7, 'title'\n",
    "metadata = load_data(N,f_metadata)\n",
    "sentences, labels, label_dict_old = process_data(metadata, data_type=data_type)\n",
    "len(label_dict_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'astro-ph': 'astro-ph',\n",
       " 'cond-mat': 'cond-mat',\n",
       " 'cs': 'cs',\n",
       " 'gr-qc': 'gr-qc',\n",
       " 'hep-ex': 'hep-ex',\n",
       " 'hep-lat': 'hep-lat',\n",
       " 'hep-ph': 'hep-ph',\n",
       " 'hep-th': 'hep-th',\n",
       " 'math-ph': 'math-ph',\n",
       " 'nlin': 'nlin',\n",
       " 'nucl-ex': 'nucl-ex',\n",
       " 'nucl-th': 'nucl-th',\n",
       " 'physics': 'physics',\n",
       " 'quant-ph': 'quant-ph',\n",
       " 'math': 'math',\n",
       " 'q-bio': 'q-bio',\n",
       " 'q-fin': 'q-fin',\n",
       " 'stat': 'stat',\n",
       " 'eess': 'eess',\n",
       " 'econ': 'econ',\n",
       " 'acc-phys': 'physics.acc-ph',\n",
       " 'adap-org': 'nlin.AO',\n",
       " 'alg-geom': 'math.AG',\n",
       " 'ao-sci': 'physics.ao-ph',\n",
       " 'atom-ph': 'physics.atom-ph',\n",
       " 'bayes-an': 'physics.data-an',\n",
       " 'chao-dyn': 'nlin.CD',\n",
       " 'chem-ph': 'physics.chem-ph',\n",
       " 'cmp-lg': 'cs.CL',\n",
       " 'comp-gas': 'nlin.CG',\n",
       " 'dg-ga': 'math.DG',\n",
       " 'funct-an': 'math.FA',\n",
       " 'mtrl-th': 'cond-mat.mtrl-sci',\n",
       " 'patt-sol': 'nlin.PS',\n",
       " 'plasm-ph': 'physics.plasm-ph',\n",
       " 'q-alg': 'math.QA',\n",
       " 'solv-int': 'nlin.SI',\n",
       " 'supr-con': 'cond-mat.supr-con'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'adap-org' in label_dict_old, 'adap-org' in label_dict_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation\n",
    "\n",
    "When we truncate the titles, do we chop off the special \" [SEP] \" token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1205 16:47:50.308459 139851126974272 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/khev/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes = 43\n",
      "Tokenizing\n"
     ]
    }
   ],
   "source": [
    "N, data_type = 10**2, 'title'\n",
    "metadata = load_data(N,f_metadata)\n",
    "sentences, labels, label_dict = process_data_sub(metadata, data_type=data_type)\n",
    "print('Num classes = {}'.format(len(label_dict)))\n",
    "print('Tokenizing')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512  # BERT pretrained model width\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts] #bert tokenizer\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\") #pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "       101, 101, 101, 101, 101, 101, 101, 101, 101, 101])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the pretrained BERT used MAX_LEN = 512, so we're stuck with this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
